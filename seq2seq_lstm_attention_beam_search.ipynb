{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq LSTM with Attention and Beam Search for English-Vietnamese Translation\n",
    "\n",
    "This notebook implements a seq2seq model with LSTM and Bahdanau attention for English-Vietnamese translation, using BPE tokenization (via `bert-base-multilingual-cased`). Inference uses Beam Search (beam_width=5) for better translation quality. The model is lightweight for RTX 3090 (24GB VRAM). Both English and Vietnamese sentences are filtered to < 50 tokens.\n",
    "\n",
    "## Steps:\n",
    "1. Load and preprocess data with BPE tokenization, filter both languages.\n",
    "2. Define model architecture (LSTM + Bahdanau Attention).\n",
    "3. Set up training (configs, optimizer, loss, BLEU metric).\n",
    "4. Train the model.\n",
    "5. Inference with Beam Search and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.7.0+cu126)\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: evaluate in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: sacrebleu in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: portalocker in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sacrebleu) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sacrebleu) (5.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from portalocker->sacrebleu) (305.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers datasets evaluate sacrebleu numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "Load raw dataset, apply BPE tokenization, filter both English and Vietnamese sentences (< 50 tokens), and create DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (571 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Paths to raw dataset\n",
    "train_en = 'detokenization/train/train.en'\n",
    "train_vi = 'detokenization/train/train.vi'\n",
    "dev_en = 'detokenization/dev/dev.en'\n",
    "dev_vi = 'detokenization/dev/dev.vi'\n",
    "test_en = 'detokenization/test/test.en'\n",
    "test_vi = 'detokenization/test/test.vi'\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Read dataset\n",
    "def read_data(en_path, vi_path):\n",
    "    with open(en_path, 'r', encoding='utf-8') as f:\n",
    "        en_data = f.readlines()\n",
    "    with open(vi_path, 'r', encoding='utf-8') as f:\n",
    "        vi_data = f.readlines()\n",
    "    return [(en.strip(), vi.strip()) for en, vi in zip(en_data, vi_data)]\n",
    "\n",
    "train_data = read_data(train_en, train_vi)\n",
    "dev_data = read_data(dev_en, dev_vi)\n",
    "test_data = read_data(test_en, test_vi)\n",
    "\n",
    "# Filter by sentence length (< 50 tokens for both languages)\n",
    "max_len = 50\n",
    "train_data = [(en, vi) for en, vi in train_data if len(tokenizer.tokenize(en)) < max_len and len(tokenizer.tokenize(vi)) < max_len][:200000]  # 200,000 pairs\n",
    "dev_data = [(en, vi) for en, vi in dev_data if len(tokenizer.tokenize(en)) < max_len and len(tokenizer.tokenize(vi)) < max_len][:10000]  # 10,000 for validation\n",
    "test_data = [(en, vi) for en, vi in test_data if len(tokenizer.tokenize(en)) < max_len and len(tokenizer.tokenize(vi)) < max_len][:5000]  # 5,000 for testing\n",
    "\n",
    "# Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=50):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        en, vi = self.data[idx]\n",
    "        en_tokens = self.tokenizer(en, max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        vi_tokens = self.tokenizer(vi, max_length=self.max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        return en_tokens['input_ids'].squeeze(0), vi_tokens['input_ids'].squeeze(0)\n",
    "\n",
    "# DataLoader\n",
    "def collate_fn(batch):\n",
    "    en_batch, vi_batch = zip(*batch)\n",
    "    en_batch = torch.stack(en_batch)\n",
    "    vi_batch = torch.stack(vi_batch)\n",
    "    return en_batch, vi_batch\n",
    "\n",
    "train_dataset = TranslationDataset(train_data, tokenizer, max_len)\n",
    "dev_dataset = TranslationDataset(dev_data, tokenizer, max_len)\n",
    "test_dataset = TranslationDataset(test_data, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture\n",
    "Seq2Seq model with LSTM encoder and decoder, using Bahdanau attention. Lightweight for RTX 3090."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
    "        energy = torch.tanh(self.Wa(hidden) + self.Ua(encoder_outputs))\n",
    "        scores = self.Va(energy).squeeze(-1)\n",
    "        attn_weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attn_weights\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
    "        cell = torch.cat((cell[-2], cell[-1]), dim=1)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size + hidden_size * 2, hidden_size * 2, num_layers, batch_first=True)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(tgt)\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        lstm_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "        output = self.fc(output.squeeze(1))\n",
    "        return output, hidden.squeeze(0), cell.squeeze(0), attn_weights\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        outputs = torch.zeros(batch_size, tgt_len, len(tokenizer)).to(device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "        input = tgt[:, 0].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell, _ = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Setup\n",
    "Define configs, optimizer, loss function, and BLEU metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configs\n",
    "vocab_size = len(tokenizer)\n",
    "embed_size = 128  # Lightweight for RTX 3090\n",
    "hidden_size = 256  # Lightweight for RTX 3090\n",
    "num_layers = 1\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_size, hidden_size, num_layers)\n",
    "decoder = Decoder(vocab_size, embed_size, hidden_size, num_layers)\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# BLEU metric\n",
    "bleu = evaluate.load('sacrebleu')\n",
    "\n",
    "# Beam Search inference function\n",
    "def translate_sentence(model, src, tokenizer, beam_width=5, max_len=50):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src)\n",
    "\n",
    "        # Initialize beam\n",
    "        beams = [(torch.tensor([[tokenizer.cls_token_id]], dtype=torch.long).to(device), 0.0, hidden, cell)]\n",
    "        completed = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for input, score, h, c in beams:\n",
    "                if input[0, -1].item() == tokenizer.sep_token_id:\n",
    "                    completed.append((input, score))\n",
    "                    continue\n",
    "\n",
    "                output, new_hidden, new_cell, _ = model.decoder(input[:, -1:], h, c, encoder_outputs)\n",
    "                probs = torch.log_softmax(output, dim=-1).squeeze(1)  # (batch_size=1, vocab_size)\n",
    "                top_probs, top_idx = probs.topk(beam_width, dim=-1)\n",
    "\n",
    "                for i in range(beam_width):\n",
    "                    new_input = torch.cat([input, top_idx[:, i:i+1]], dim=1)\n",
    "                    new_score = score + top_probs[:, i].item()\n",
    "                    new_beams.append((new_input, new_score, new_hidden, new_cell))\n",
    "\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            if len(completed) >= beam_width:\n",
    "                break\n",
    "\n",
    "        # Select best completed sequence or top beam if none completed\n",
    "        if completed:\n",
    "            best_sequence = max(completed, key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            best_sequence = beams[0][0]\n",
    "\n",
    "        return tokenizer.decode(best_sequence[0, 1:], skip_special_tokens=True)\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in tqdm(dataloader, desc='Training'):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[:, 1:].reshape(-1, output.size(-1))\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Validation with BLEU (using Beam Search)\n",
    "def evaluate_bleu(model, dataloader, tokenizer, beam_width=5, max_len=50):\n",
    "    model.eval()\n",
    "    predictions, references = [], []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc='Evaluating'):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            for i in range(src.size(0)):\n",
    "                pred_text = translate_sentence(model, src[i:i+1], tokenizer, beam_width, max_len)\n",
    "                ref_text = tokenizer.decode(tgt[i, 1:], skip_special_tokens=True)\n",
    "                predictions.append(pred_text)\n",
    "                references.append([ref_text])\n",
    "    return bleu.compute(predictions=predictions, references=references)['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "Train for 10 epochs, validate after each epoch, save best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [47:37<00:00,  2.19it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [15:16<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 4.8506, BLEU = 7.40\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [48:47<00:00,  2.13it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [15:23<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 3.9536, BLEU = 10.01\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [48:51<00:00,  2.13it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [15:40<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 3.6231, BLEU = 11.32\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [48:51<00:00,  2.13it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [15:44<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 3.4305, BLEU = 12.21\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [48:48<00:00,  2.13it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [15:49<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 3.2906, BLEU = 12.60\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [48:48<00:00,  2.13it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [16:02<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 3.1844, BLEU = 13.23\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [48:46<00:00,  2.14it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [16:06<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 3.0990, BLEU = 13.46\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [48:51<00:00,  2.13it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [15:39<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 3.0290, BLEU = 13.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [47:39<00:00,  2.19it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [14:27<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 2.9664, BLEU = 13.67\n",
      "Saved best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 6250/6250 [46:49<00:00,  2.22it/s]\n",
      "Evaluating: 100%|██████████| 313/313 [13:27<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 2.9147, BLEU = 14.03\n",
      "Saved best model\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_bleu = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    bleu_score = evaluate_bleu(model, dev_loader, tokenizer)\n",
    "    print(f'Epoch {epoch+1}: Loss = {train_loss:.4f}, BLEU = {bleu_score:.2f}')\n",
    "    if bleu_score > best_bleu:\n",
    "        best_bleu = bleu_score\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print('Saved best model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference\n",
    "Translate using Beam Search and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Spokesperson for the European Union External Action Services.\n",
      "Translation: Chân mật của Von Kính Lực Minh Kiểm Minh Ki.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 157/157 [07:37<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU: 16.49\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Test sample translation\n",
    "sample_sentence = 'Spokesperson for the European Union External Action Services.'\n",
    "tokens = tokenizer(sample_sentence, max_length=max_len, padding='max_length', truncation=True, return_tensors='pt')\n",
    "translated = translate_sentence(model, tokens['input_ids'], tokenizer)\n",
    "print(f'Input: {sample_sentence}')\n",
    "print(f'Translation: {translated}')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_bleu = evaluate_bleu(model, test_loader, tokenizer)\n",
    "print(f'Test BLEU: {test_bleu:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

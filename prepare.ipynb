{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f549e38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bff784",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove redundant spaces\n",
    "    text = re.sub(r\"&[^;]+;\", \"\", text)  # Remove HTML entities\n",
    "    text = re.sub(r\"[<>\\\\\\[\\]{}~^|]\", \"\", text)  # Remove noisy chars\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_pair(en, vi, min_len=3, max_len=100):\n",
    "    if not en or not vi:\n",
    "        return False\n",
    "    if len(en.split()) < min_len or len(vi.split()) < min_len:\n",
    "        return False\n",
    "    if len(en.split()) > max_len or len(vi.split()) > max_len:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(folder, split_name):\n",
    "    en_path = os.path.join(folder, f\"{split_name}.en\")\n",
    "    vi_path = os.path.join(folder, f\"{split_name}.vi\")\n",
    "\n",
    "    with open(en_path, encoding=\"utf-8\") as f:\n",
    "        en_lines = [line.strip() for line in f]\n",
    "    with open(vi_path, encoding=\"utf-8\") as f:\n",
    "        vi_lines = [line.strip() for line in f]\n",
    "\n",
    "    assert len(en_lines) == len(vi_lines), f\"‚ùå Mismatch in line count for {split_name}\"\n",
    "\n",
    "    data = []\n",
    "    for en, vi in zip(en_lines, vi_lines):\n",
    "        en_clean = normalize_text(en)\n",
    "        vi_clean = normalize_text(vi)\n",
    "        if is_valid_pair(en_clean, vi_clean):\n",
    "            data.append({\n",
    "                \"translation\": {\n",
    "                    \"en\": en_clean,\n",
    "                    \"vi\": vi_clean\n",
    "                }\n",
    "            })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_phomt(root_folder, output_path=\"phomt_cleaned.json\", combine_all=True):\n",
    "    print(f\"üìÅ Loading PhoMT from: {root_folder}\")\n",
    "    splits = [\"train\", \"dev\", \"test\"]\n",
    "    data_all = []\n",
    "\n",
    "    for split in splits:\n",
    "        folder_path = os.path.join(root_folder, \"detokenization\", split)\n",
    "        print(f\"üîπ Processing split: {split}\")\n",
    "        split_data = load_split(folder_path, split)\n",
    "        print(f\"‚úÖ {split}: {len(split_data)} valid pairs\")\n",
    "\n",
    "        if combine_all:\n",
    "            data_all.extend(split_data)\n",
    "        else:\n",
    "            out_file = f\"{split}.json\" # out_file = f\"{output_path.replace('.json', '')}_{split}.json\"\n",
    "            with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(split_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"üíæ Saved: {out_file}\")\n",
    "\n",
    "    if combine_all:\n",
    "        print(f\"\\nüì¶ Total cleaned pairs: {len(data_all)}\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data_all, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Saved combined file to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prepare_phomt(\n",
    "        root_folder=r\"D:\\PhoMT\",  # ‚úÖ ƒë∆∞·ªùng d·∫´n g·ªëc ch·ª©a folder \"detokenization\"\n",
    "        output_path=\"phomt_cleaned.json\",\n",
    "        combine_all=True  # N·∫øu mu·ªën chia nh·ªè theo split th√¨ ƒë·∫∑t False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
